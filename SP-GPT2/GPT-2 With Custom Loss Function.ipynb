{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "GPT-2 With Custom Loss Function.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zNoqdfS2pUfo",
    "outputId": "54f18e31-6177-4ecd-c5af-dbed809de3be"
   },
   "source": [
    "!nvidia-smi"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Tue Jun  8 03:45:19 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   44C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "f9Pv1i7j2hLZ"
   },
   "source": [
    "%%capture\n",
    "!pip install transformers\n",
    "!pip install einops"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mpdoUppdDg7e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "54cdea3b-e0b7-4cf0-d783-8f4eb0e1aa46"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "t_zaS4DBDvgu"
   },
   "source": [
    "%%capture\n",
    "%cd '/content/drive/MyDrive'\n",
    "!ls"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nobBd8zqCenM"
   },
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import RobertaTokenizerFast\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "from filelock import FileLock\n",
    "from transformers.utils import logging\n",
    "from typing import Dict, List, Optional\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "import math"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ftiqU9PkyGrn"
   },
   "source": [
    "class ScaleDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaleDotProductAttention, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None, e=1e-12):\n",
    "        batch_size, head, length, d_tensor = k.size()\n",
    "\n",
    "        score = torch.einsum(\"bhid,bhjd->bhij\",q,k)\n",
    "        score = score/math.sqrt(d_tensor)\n",
    "\n",
    "        if mask is not None:\n",
    "            score = score.masked_fill(mask == 0, -e)\n",
    "\n",
    "        score = self.softmax(score)\n",
    "\n",
    "        v = score @ v\n",
    "\n",
    "        return v, score"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Q5rBQArHyJ10"
   },
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, n_head):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.attention = ScaleDotProductAttention()\n",
    "        self.w_q = nn.Linear(d_model, d_model*n_head)\n",
    "        self.w_k = nn.Linear(d_model, d_model*n_head)\n",
    "        self.w_v = nn.Linear(d_model, d_model*n_head)\n",
    "        self.w_concat = nn.Linear(d_model*n_head, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        q, k, v = self.w_q(x), self.w_k(x), self.w_v(x)\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.n_head), (q, k, v))\n",
    "\n",
    "        out, attention = self.attention(q, k, v, mask=mask)\n",
    "\n",
    "        # 4. concat and pass to linear layer\n",
    "        # out = self.concat(out)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.w_concat(out)\n",
    "\n",
    "        return out"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XFw91tT0eAhR"
   },
   "source": [
    "class SelfAttentionLstm(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers,n_head):\n",
    "        super(SelfAttentionLstm, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.multi_attention = MultiHeadAttention(d_model=input_size,n_head=4)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.multi_attention(x)\n",
    "         \n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(\"cuda\")\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to('cuda')\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = out[: ,-1, : ]\n",
    "        return out"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "C8cfdlX2E8Lc"
   },
   "source": [
    "train_path = 'Data/Poem/train_dataset_27_04.txt'\n",
    "test_path = 'Data/Poem/valid_dataset_27_04.txt'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0Tjjj_r-KWTv"
   },
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"./Tokenizer_27_04\", max_len=512)\n",
    "tokenizer.add_tokens('\\n')\n",
    "vocab_size= tokenizer.vocab_size\n",
    "vocab_size = vocab_size + 1"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BJFC4l3_iBc_",
    "outputId": "d6157a7e-bc64-4edb-96f1-8280026ca435"
   },
   "source": [
    "vocab_size"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "14673"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 11
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SCUlkmXlhSWv"
   },
   "source": [
    "def add_padding(list_token: list, block_size:int):\n",
    "    tmp_list = [0]* block_size\n",
    "    tmp_list[0:len(list_token)] = list_token\n",
    "    tmp_list[len(list_token):block_size] = [1]*(block_size-len(list_token))\n",
    "    return tmp_list"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ftqTXKGQE9As"
   },
   "source": [
    "logger = logging.get_logger(__name__)\n",
    "class CusTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This will be superseded by a framework-agnostic approach\n",
    "    soon.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        file_path: str,\n",
    "        block_size: int,\n",
    "        overwrite_cache=False,\n",
    "        cache_dir: Optional[str] = None,\n",
    "    ):\n",
    "        assert os.path.isfile(file_path), f\"Input file path {file_path} not found\"\n",
    "\n",
    "        directory, filename = os.path.split(file_path)\n",
    "        cached_features_file = os.path.join(\n",
    "            cache_dir if cache_dir is not None else directory,\n",
    "            \"cached_lm_{}_{}_{}\".format(\n",
    "                tokenizer.__class__.__name__,\n",
    "                str(block_size),\n",
    "                filename,\n",
    "            ),\n",
    "        )\n",
    "        lock_path = cached_features_file + \".lock\"\n",
    "        with FileLock(lock_path):\n",
    "\n",
    "            if os.path.exists(cached_features_file) and not overwrite_cache:\n",
    "                start = time.time()\n",
    "                with open(cached_features_file, \"rb\") as handle:\n",
    "                    self.examples = pickle.load(handle)\n",
    "                logger.info(\n",
    "                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                logger.info(f\"Creating features from dataset file at {directory}\")\n",
    "\n",
    "                self.examples = []\n",
    "                with open(file_path, encoding=\"utf-8\") as f:\n",
    "                    total_poem = f.read()\n",
    "                split_total_poem = total_poem.split(\"\\n\\n\")\n",
    "                canto_poem = [split_total_poem[x:x+4] for x in range(0, len(split_total_poem), 4)]\n",
    "                canto_poem = [\"\\n\\n\".join(i) for i in canto_poem]\n",
    "\n",
    "                canto_token = [tokenizer.encode(i) for i in canto_poem]\n",
    "                canto_token = [i for i in canto_token if len(i) >= 129 and len(i) <= 140]\n",
    "\n",
    "                for i in canto_token:\n",
    "                  self.examples.append(add_padding(i,block_size=block_size ))\n",
    "\n",
    "\n",
    "                start = time.time()\n",
    "                with open(cached_features_file, \"wb\") as handle:\n",
    "                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                logger.info(\n",
    "                    \"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n",
    "                )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i) -> torch.Tensor:\n",
    "        return torch.tensor(self.examples[i], dtype=torch.long)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FIVvq4REFJUg"
   },
   "source": [
    "def load_dataset(train_path, test_path, tokenizer):\n",
    "    train_dataset = CusTextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=140)\n",
    "     \n",
    "    test_dataset = CusTextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=140)   \n",
    "    \n",
    "    return train_dataset,test_dataset\n",
    "\n",
    "train_dataset,test_dataset = load_dataset(train_path,test_path, tokenizer)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9u8pgwe0TCy"
   },
   "source": [
    "#Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PxVk83M7HEIj"
   },
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=8, shuffle=False)\n",
    "test_loader = DataLoader(dataset= test_dataset, batch_size= 8, shuffle=False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CUuttJGKJu87"
   },
   "source": [
    "from transformers import Trainer, TrainingArguments, GPT2Config, GPT2LMHeadModel,GPT2Model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fjGoC3HkJxAj"
   },
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "configuration = GPT2Config(vocab_size=vocab_size,n_layer = 8)\n",
    "poem = GPT2LMHeadModel(configuration).to(\"cuda\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jI2TySj0LW7"
   },
   "source": [
    "#Train GPT-2\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cgvZE9dlH7cD"
   },
   "source": [
    "lr_rate = 3e-5\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(poem.parameters(), lr_rate)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "A8Dta5PskWfH"
   },
   "source": [
    "def save_checkpoint(state, filename= \"GPT-2/gpt_2_custom_loss_v2.pth.tar\"):\n",
    "    print(\"Saving checkpoint\")\n",
    "    torch.save(state,filename)\n",
    "\n",
    "def load_checkpoint(state):\n",
    "    print(\"Load checkpoint\")\n",
    "    poem.load_state_dict(state['state_dict'])\n",
    "    optimizer.load_state_dict(state['optimizer'])\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9dq8_yR4nis",
    "outputId": "05bd7cf0-d801-4b31-bddc-0299e1628a1f"
   },
   "source": [
    "load_checkpoint(torch.load(\"GPT-2/gpt_2_27_04_dataset.pth.tar\"))"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Load checkpoint\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dbDeIF8BZhY2"
   },
   "source": [
    "head_gpt = SelfAttentionLstm(input_size=768,hidden_size=800, num_layers=2,n_head=4).to('cuda')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wzyTlSgDRpoL"
   },
   "source": [
    "def custom_index(list_token:list):\n",
    "    list_token = [list_token[i:i+4] for i in range(0,len(list_token),4)]\n",
    "    for i in range(len(list_token)):\n",
    "      list_token[i] = [list_token[i][0],list_token[i][3]]\n",
    "    return list_token"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yehdU-AARLG0"
   },
   "source": [
    "def get_idx_two_line(lm_logits):\n",
    "    token = torch.argmax(lm_logits, dim= 2)\n",
    "    token = token[0].tolist()\n",
    "    index_token = [0]\n",
    "    for i in range(len(token)):\n",
    "        if token[i:i+2] == [vocab_size-1,vocab_size-1]:\n",
    "          index_token.append(i)\n",
    "          index_token.append(i+2)\n",
    "    index_token.append(len(token))\n",
    "\n",
    "    # Lấy index đầu và cuối của 1 khổ\n",
    "    index_khotho = [index_token[i:i+2] for i in range(0,len(index_token),2)]\n",
    "    index_khotho = [i for i in index_khotho if len(i) == 2]\n",
    "\n",
    "    a = index_khotho\n",
    "    \n",
    "    #Lấy index của token đầu và token cuối của 2 câu trong 1 khổ\n",
    "    token_final = []\n",
    "    for idx_khotho in index_khotho:\n",
    "        tmp = token[idx_khotho[0]:idx_khotho[1]]\n",
    "        token_tmp = [idx_khotho[0]]\n",
    "        for i in range(len(tmp)):\n",
    "          if tmp[i] == vocab_size-1:\n",
    "            token_tmp.append(i + idx_khotho[0])\n",
    "            token_tmp.append(i+1 +idx_khotho[0])\n",
    "        token_tmp.append(idx_khotho[1])\n",
    "        if len(token_tmp) != 8:\n",
    "          continue \n",
    "        else :\n",
    "          token_final.append(custom_index(token_tmp))\n",
    "    \n",
    "    return token_final"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qU1DUX8UUCWI"
   },
   "source": [
    "def loss_kho_tho(lm_logits,embedding):\n",
    "    lm_logits = torch.unsqueeze(lm_logits,0)\n",
    "    pair_list = get_idx_two_line(lm_logits)\n",
    "    embedding = torch.unsqueeze(embedding,0)\n",
    "    \n",
    "    total_lost = 0\n",
    "    loss = nn.MSELoss().to(device)\n",
    "    for pair in pair_list:\n",
    "        one = pair[0]\n",
    "        two = pair[1]\n",
    "\n",
    "        if one == None or two == None:\n",
    "          continue\n",
    "\n",
    "        embedd_one = head_gpt(embedding[:,one[0]: one[1], :])\n",
    "        embedd_two = head_gpt(embedding[:,two[0]: two[1], :])\n",
    "\n",
    "        total_lost += loss(embedd_one,embedd_two)\n",
    "\n",
    "    return total_lost     "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "W2XLvo--hFN1"
   },
   "source": [
    "for param in head_gpt.parameters():\n",
    "    param.require_grad = True\n",
    "\n",
    "for param in poem.parameters():\n",
    "    param.require_grad = True"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Dclk945MblaA"
   },
   "source": [
    " def train(model, train_loader, device, criterion, vocab_size, optimizer,num_epochs):\n",
    "    list_loss = []\n",
    "    n_batches, n_samples = len(train_loader), len(train_loader.dataset)\n",
    "    \n",
    "    model.train()\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    for index_epoch in range(num_epochs):\n",
    "      losses = 0\n",
    "      for i, batch in enumerate(train_loader):\n",
    "        inputs = batch.to(device)\n",
    "        targets = inputs[:, 1:].contiguous()\n",
    "        \n",
    "        lm_logits = model(inputs).logits \n",
    "        embedding = model.transformer(inputs)[0]\n",
    "        lm_logits_1 = lm_logits[:, :-1].contiguous()\n",
    "\n",
    "        loss = criterion(lm_logits_1.view(-1, vocab_size), targets.view(-1))\n",
    "        loss = loss + sum([loss_kho_tho(lm_logits[i],embedding[i]) for i in range(lm_logits.shape[0])])*5\n",
    "        print('index: {}, loss: {}'.format(i, loss))\n",
    "        losses += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "      list_loss.append(losses)\n",
    "      print('='*20)\n",
    "      print('epoch: {}'.format(index_epoch))\n",
    "\n",
    "      checkpoint = {'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
    "      save_checkpoint(checkpoint, filename= \"GPT-2/gpt_2_custom_loss_v2.pth.tar\")\n",
    "      print('Loss: {}'.format(list_loss))\n",
    "      \n",
    "train(poem, train_loader, device, criterion, vocab_size, optimizer, num_epochs= 100)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iIgU5LOn4w6r"
   },
   "source": [
    "class TextGenerator():\n",
    "\n",
    "    def __init__(self, max_tokens, start_tokens, maxlen, model, tokenizer,device, topk):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.start_tokens = start_tokens\n",
    "        self.maxlen = maxlen\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.k = topk \n",
    "\n",
    "    def sample_from(self, logits):\n",
    "        logits, indices = torch.topk(logits, k=self.k, sorted=True)\n",
    "        return np.random.choice(indices.cpu().numpy())\n",
    "\n",
    "\n",
    "    def gen_poem(self):\n",
    "        start_tokens = [_ for _ in self.start_tokens]\n",
    "        num_tokens_generated = 0\n",
    "        tokens_generated = []\n",
    "        while num_tokens_generated <= self.max_tokens:\n",
    "            pad_len = self.maxlen - len(start_tokens)\n",
    "            sample_index = len(start_tokens) - 1\n",
    "            if pad_len < 0:\n",
    "                x = start_tokens[:self.maxlen]\n",
    "                sample_index = self.maxlen - 1\n",
    "            elif pad_len > 0:\n",
    "                x = start_tokens + [0] * pad_len\n",
    "            else:\n",
    "                x = start_tokens\n",
    "            x = torch.tensor([x], device= self.device)\n",
    "            y = self.model(x).logits\n",
    "            sample_token = self.sample_from(y[0][sample_index])\n",
    "            tokens_generated.append(sample_token)\n",
    "            start_tokens.append(sample_token)\n",
    "            num_tokens_generated = len(tokens_generated)\n",
    "            # print(sample_token)\n",
    "        output_token = [_ for _ in self.start_tokens + tokens_generated]\n",
    "        poem = self.tokenizer.decode(output_token)\n",
    "        print(f\"generated text:\\n{poem}\\n\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MkGjagmZyMG0",
    "outputId": "f51ef611-1428-42b9-d736-113515a9af65"
   },
   "source": [
    "num_token_generated = 30\n",
    "hint = 'mùa thu'\n",
    "start_tokens = tokenizer.encode(hint)[:-1]\n",
    "generator = TextGenerator(max_tokens= num_token_generated, start_tokens= start_tokens, maxlen= 300, model= poem, tokenizer= tokenizer,device= device, topk= 1)\n",
    "generator.gen_poem()"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "generated text:\n",
      "<s> mùa thu năm ấy gặp em \n",
      " nắng chiều ấm áp nắng oi dòng rồi \n",
      " còn không em nữa đôi môi \n",
      " nụ cười e thẹn đôi môi ngập ngừng \n",
      "\n",
      "\n"
     ],
     "name": "stdout"
    }
   ]
  }
 ]
}